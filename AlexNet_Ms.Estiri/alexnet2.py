# -*- coding: utf-8 -*-
"""Copy of INQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j1mh5q1HsZrTBu-0yraNlUMTRlwnJxAo
"""



from keras.datasets import cifar10
from keras.utils import np_utils
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D
from keras.layers.normalization import BatchNormalization
from keras.optimizers import rmsprop
from keras.regularizers import l2
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD
from keras.optimizers import Optimizer
import random
import math

from tensorflow.python.client import device_lib
#print(device_lib.list_lo             cal_devices())

from keras import backend as K
from keras.legacy import interfaces
K.tensorflow_backend._get_available_gpus()

NO_EPOCHS = 1

CLASSES = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog",
           "horse", "ship", "truck"]



def get_class_name (array):
  
  array2= []
  for i in array :
    array2.append(CLASSES[i])
    
  return array2

def get_data(num_classes=10):

  print('[INFO] Loading the CIFAR10 dataset...')
  (train_data, train_labels), (test_data, test_labels) = cifar10.load_data()

  train_labels = np_utils.to_categorical(train_labels, num_classes)
  test_labels = np_utils.to_categorical(test_labels, num_classes)
  #print (train_labels)
  #print (test_labels)
 
  train_data = train_data.astype('float32')
  test_data = test_data.astype('float32')
  train_data /= 255
  test_data /= 255

  return train_data, train_labels, test_data, test_labels

train_data, train_labels, test_data, test_labels = get_data()

#get_data(num_classes=10)
train_datagen = ImageDataGenerator(
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True)   # flip images horizontally

validation_datagen = ImageDataGenerator()

train_generator = train_datagen.flow(train_data[:40000],train_labels[:40000], batch_size=32)
validation_generator = validation_datagen.flow(train_data[40000:], train_labels[40000:], batch_size=32)



def alexnet_model(img_shape=(32, 32, 3),n_classes=10, l2_reg=0.01 ,weights=None):

  
  alexnet = Sequential()

  # Layer 1
  alexnet.add(Conv2D(96, (11, 11), input_shape=img_shape,
    padding='same', kernel_regularizer=l2(l2_reg)))
  
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(MaxPooling2D(pool_size=(2, 2)))
  alexnet.add(Dropout(0.25))

  # Layer 2
  alexnet.add(Conv2D(256, (5, 5), padding='same',name ='convolution'))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(MaxPooling2D(pool_size=(2, 2)))

  # Layer 3
  alexnet.add(ZeroPadding2D((1, 1)))
  alexnet.add(Conv2D(384, (3, 3), padding='same'))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(MaxPooling2D(pool_size=(2, 2)))

  # Layer 4
  alexnet.add(ZeroPadding2D((1, 1)))
  alexnet.add(Conv2D(384, (3, 3), padding='same'))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))

  # Layer 5
  alexnet.add(ZeroPadding2D((1, 1)))
  alexnet.add(Conv2D(256, (3, 3), padding='same'))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(MaxPooling2D(pool_size=(2, 2)))

  # Layer 6
  alexnet.add(Flatten())
  alexnet.add(Dense(4096))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(Dropout(0.5))

  # Layer 7
  alexnet.add(Dense(4096))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(Dropout(0.5))
  
  # Layer 8
  alexnet.add(Dense(n_classes))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('softmax'))

  if weights is not None:
    alexnet.load_weights(weights)

  return alexnet

model = alexnet_model()
model.summary()

#print((model.layers[0].weights[0][0][0][0][0]))
#print((model.layers))

#print (model.get_layer("conv2d_1").get_weights())
#w1 = model.get_layer("conv2d_1").get_weights()

model.compile(loss='categorical_crossentropy',
 		optimizer=rmsprop(lr=0.0001, decay=1e-6),
     metrics=['accuracy'])
# mo = MyOptimizer(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
# model.compile(loss='categorical_crossentropy',
# 		optimizer=mo,
#     metrics=['accuracy'])

#train_data, train_labels, test_data, test_labels = get_data()

'''history = model.fit(train_data, train_labels,
			batch_size=128,
			epochs=NO_EPOCHS,
			validation_data=(test_data, test_labels),
			verbose=True,
      shuffle=True)'''
# fits the model on batches with real-time data augmentation:
history = model.fit_generator(train_generator,    
                    validation_data=validation_generator,
                    validation_steps=len(train_data[40000:]) / 32,
                    steps_per_epoch=len(train_data[:40000]) / 32,
                    epochs=2,
                    verbose=2)

(loss, accuracy) = model.evaluate(test_data, test_labels,
			batch_size=128,
			verbose=1)
print('[INFO] accuracy: {:.2f}%'.format(accuracy * 100))

#img=mpimg.imread('/content/t1.png')
#img22=mpimg.imread('/content/t2.png')




#img2 = np.array([img, img22])



#a = model.predict_classes(img2)

#print (get_class_name(a))

# from google.colab import drive
# drive.mount('/content/gdrive')
# tf.image_summary(train_data)

model.save_weights("/content/w9epoch")
#model.save_weights("/content/gdrive/My Drive/w9epoch")