# -*- coding: utf-8 -*-
"""Copy of alexnet_first.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WOY8NyiezOogac7lNZs9m7da1a8qbKXp
"""

from keras.datasets import cifar10
from keras.utils import np_utils
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D
from keras.layers.normalization import BatchNormalization
from keras.optimizers import rmsprop
from keras.regularizers import l2
import matplotlib.pyplot as plt

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

NO_EPOCHS = 20

def get_data(num_classes=10):

  print('[INFO] Loading the CIFAR10 dataset...')
  (train_data, train_labels), (test_data, test_labels) = cifar10.load_data()

  # Transform labels to one hot labels
  # Example: '0' will become [1, 0, 0, 0, 0, 0, 0, 0, 0]
  #          '1' will become [0, 1, 0, 0, 0, 0, 0, 0, 0]
  #          and so on...
  train_labels = np_utils.to_categorical(train_labels, num_classes)
  test_labels = np_utils.to_categorical(test_labels, num_classes)

  # Change type and normalize data
  train_data = train_data.astype('float32')
  test_data = test_data.astype('float32')
  train_data /= 255
  test_data /= 255

  return train_data, train_labels, test_data, test_labels

def draw_training_curve(history):
  plt.figure(1)

  # History for accuracy
  plt.subplot(211)
  plt.plot(history.history['acc'])
  plt.plot(history.history['val_acc'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')

  # History for loss
  plt.subplot(212)
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')

  plt.show()

def alexnet_model(img_shape=(224, 224, 3),n_classes=10, l2_reg=0. ,weights=None):

  # Initialize model
  alexnet = Sequential()

  # Layer 1
  alexnet.add(Conv2D(96, (11, 11), input_shape=img_shape,
    padding='same', kernel_regularizer=l2(l2_reg)))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(MaxPooling2D(pool_size=(2, 2)))

  # Layer 2
  alexnet.add(Conv2D(256, (5, 5), padding='same'))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(MaxPooling2D(pool_size=(2, 2)))

  # Layer 3
  alexnet.add(ZeroPadding2D((1, 1)))
  alexnet.add(Conv2D(512, (3, 3), padding='same'))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(MaxPooling2D(pool_size=(2, 2)))

  # Layer 4
  alexnet.add(ZeroPadding2D((1, 1)))
  alexnet.add(Conv2D(1024, (3, 3), padding='same'))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))

  # Layer 5
  alexnet.add(ZeroPadding2D((1, 1)))
  alexnet.add(Conv2D(1024, (3, 3), padding='same'))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(MaxPooling2D(pool_size=(2, 2)))

  # Layer 6
  alexnet.add(Flatten())
  alexnet.add(Dense(3072))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(Dropout(0.5))

  # Layer 7
  alexnet.add(Dense(4096))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('relu'))
  alexnet.add(Dropout(0.5))

  # Layer 8
  alexnet.add(Dense(n_classes))
  alexnet.add(BatchNormalization())
  alexnet.add(Activation('softmax'))

  if weights is not None:
    alexnet.load_weights(weights)

  return alexnet

model = alexnet_model(img_shape=(32, 32 , 3))
model.summary()

model.compile(loss='categorical_crossentropy',
		optimizer=rmsprop(lr=0.0001, decay=1e-6),
    metrics=['accuracy'])

train_data, train_labels, test_data, test_labels = get_data()

print(train_data.shape)

history = model.fit(train_data, train_labels,
			batch_size=100,
			epochs=NO_EPOCHS,
			validation_data=(test_data, test_labels),
			verbose=True,
      shuffle=True)

(loss, accuracy) = model.evaluate(test_data, test_labels,
			batch_size=128,
			verbose=1)
print('[INFO] accuracy: {:.2f}%'.format(accuracy * 100))

draw_training_curve(history)